{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":670723,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":508012,"modelId":522688}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport math\nimport csv\nimport random\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport psutil\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tokenizers import ByteLevelBPETokenizer\nfrom transformers import GPT2TokenizerFast\nfrom datasets import load_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:43:06.418499Z","iopub.execute_input":"2025-12-03T05:43:06.418738Z","iopub.status.idle":"2025-12-03T05:43:14.430946Z","shell.execute_reply.started":"2025-12-03T05:43:06.418713Z","shell.execute_reply":"2025-12-03T05:43:14.430372Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# -------------------- USER CONFIG --------------------\nOUT_DIR = Path(\"/kaggle/working/hindi-gpt\")\nCKPT_DIR = OUT_DIR / \"checkpoints\"\nTK_DIR = OUT_DIR / \"tokenizer\"\nLOG_DIR = OUT_DIR / \"logs\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nCKPT_DIR.mkdir(exist_ok=True)\nTK_DIR.mkdir(exist_ok=True)\nLOG_DIR.mkdir(exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:56:01.166969Z","iopub.execute_input":"2025-12-03T05:56:01.167311Z","iopub.status.idle":"2025-12-03T05:56:01.173195Z","shell.execute_reply.started":"2025-12-03T05:56:01.167291Z","shell.execute_reply":"2025-12-03T05:56:01.172464Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"BLOCK_SIZE = 128    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:20:22.995372Z","iopub.execute_input":"2025-12-03T06:20:22.996175Z","iopub.status.idle":"2025-12-03T06:20:22.999297Z","shell.execute_reply.started":"2025-12-03T06:20:22.996152Z","shell.execute_reply":"2025-12-03T06:20:22.998729Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Model / tokenizer config\nTEACHER_BLOCK_SIZE = 128\nTEACHER_D_MODEL = 512\nTEACHER_N_HEAD = 8\nTEACHER_N_LAYER = 8\nTEACHER_DROPOUT = 0.1\nTEACHER_VOCAB_SIZE = 32000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:46:21.537534Z","iopub.execute_input":"2025-12-03T05:46:21.537871Z","iopub.status.idle":"2025-12-03T05:46:21.542101Z","shell.execute_reply.started":"2025-12-03T05:46:21.537848Z","shell.execute_reply":"2025-12-03T05:46:21.541385Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Model / tokenizer config (student)\nSTUDENT_BLOCK_SIZE = 128\nSTUDENT_D_MODEL = 256\nSTUDENT_N_HEAD = 4\nSTUDENT_N_LAYER = 4\nSTUDENT_DROPOUT = 0.1\nSTUDENT_VOCAB_SIZE = 32000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:11.161611Z","iopub.execute_input":"2025-12-03T06:17:11.162372Z","iopub.status.idle":"2025-12-03T06:17:11.165979Z","shell.execute_reply.started":"2025-12-03T06:17:11.162349Z","shell.execute_reply":"2025-12-03T06:17:11.165183Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"\n# Training config\nTOTAL_STEPS = 50000   \nBATCH_SIZE = 16       \nEVAL_INTERVAL = 2000\nEVAL_ITERS = 100\nLEARNING_RATE = 3e-4\nSAVE_EVERY = 2000\nSEED = 1337\n# Data selection\nTARGET_BYTES = 1 * 1024**3   # ~1 GB raw text\nMAX_DOCS = None  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:13.042783Z","iopub.execute_input":"2025-12-03T06:17:13.043046Z","iopub.status.idle":"2025-12-03T06:17:13.046968Z","shell.execute_reply.started":"2025-12-03T06:17:13.043027Z","shell.execute_reply":"2025-12-03T06:17:13.046386Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Misc\nUSE_GRAD_ACCUM = False\nGRAD_ACCUM_STEPS = 2\nGENERATE_TOKENS = 200\nINFERENCE_PROMPT = \"à¤­à¤¾à¤°à¤¤ à¤•à¥€ à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤ªà¥à¤°à¤—à¤¤à¤¿ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤µà¤¿à¤¸à¥à¤¤à¥ƒà¤¤ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥€à¤œà¤¿à¤ à¤”à¤° à¤¬à¤¤à¤¾à¤‡à¤ à¤•à¤¿ à¤†à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤µà¤°à¥à¤·à¥‹à¤‚ à¤®à¥‡à¤‚ à¤­à¤¾à¤°à¤¤ à¤•à¤¿à¤¸ à¤¦à¤¿à¤¶à¤¾ à¤®à¥‡à¤‚ à¤†à¤—à¥‡ à¤¬à¤¢à¤¼à¥‡à¤—à¤¾à¥¤\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:48:03.837903Z","iopub.execute_input":"2025-12-03T05:48:03.838433Z","iopub.status.idle":"2025-12-03T05:48:03.841991Z","shell.execute_reply.started":"2025-12-03T05:48:03.838414Z","shell.execute_reply":"2025-12-03T05:48:03.841217Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# reproducibility\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# device info\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nn_gpus = torch.cuda.device_count()\nprint(f\"Device: {device}, GPUs: {n_gpus}\")\nif device == \"cuda\":\n    for i in range(n_gpus):\n        print(\" GPU\", i, \":\", torch.cuda.get_device_name(i))\nprint(\"RAM (GB):\", psutil.virtual_memory().total / 1e9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:48:04.898198Z","iopub.execute_input":"2025-12-03T05:48:04.898692Z","iopub.status.idle":"2025-12-03T05:48:04.904724Z","shell.execute_reply.started":"2025-12-03T05:48:04.898670Z","shell.execute_reply":"2025-12-03T05:48:04.904042Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, GPUs: 1\n GPU 0 : Tesla P100-PCIE-16GB\nRAM (GB): 33.662472192\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# -------------------- LOAD DATASET (nisram) & extract ~1GB --------------------\nprint(\"Loading nis12ram/nisram-hindi-text-0.0 ...\")\nds = load_dataset(\"nis12ram/nisram-hindi-text-0.0\", split=\"train\")\n\nall_texts = ds[\"text\"]\nprint(\"Total rows in dataset:\", len(all_texts))\n\nindices = list(range(len(all_texts)))\nrandom.shuffle(indices)\n\ntexts = []\nacc_bytes = 0\ncount = 0\nfor i in indices:\n    if MAX_DOCS is not None and count >= MAX_DOCS:\n        break\n    t = all_texts[i]\n    if not t:\n        continue\n    b = t.encode(\"utf-8\")\n    acc_bytes += len(b)\n    texts.append(t)\n    count += 1\n    if count % 20000 == 0:\n        print(f\"Collected {count:,} docs, {acc_bytes/1e9:.3f} GB\")\n    if acc_bytes >= TARGET_BYTES:\n        break\n\nprint(f\"Selected {len(texts):,} documents, approx {acc_bytes/1e9:.3f} GB\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:48:19.380080Z","iopub.execute_input":"2025-12-03T05:48:19.380769Z","iopub.status.idle":"2025-12-03T05:54:37.956233Z","shell.execute_reply.started":"2025-12-03T05:48:19.380745Z","shell.execute_reply":"2025-12-03T05:54:37.955223Z"}},"outputs":[{"name":"stdout","text":"Loading nis12ram/nisram-hindi-text-0.0 ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f4d81f2fe94f6abce79ee7fc087f9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a740a6fe30d41bfba2f85f5f5169f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a978c05cc742a1a343868c27ffcfdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf96067fe4af4bb0b55b8c64cfb603ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00003-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f124de5de544452f9c138408b42bdc26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00004-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5556bed44642f4832f2dc84f1dfdb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00005-of-00006.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e835a3bc5ad248b1a799945eca5b3dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/601628 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe8cd9f39bd649aba67f9b6aaa7fded4"}},"metadata":{}},{"name":"stdout","text":"Total rows in dataset: 601628\nCollected 20,000 docs, 0.091 GB\nCollected 40,000 docs, 0.181 GB\nCollected 60,000 docs, 0.271 GB\nCollected 80,000 docs, 0.361 GB\nCollected 100,000 docs, 0.451 GB\nCollected 120,000 docs, 0.542 GB\nCollected 140,000 docs, 0.632 GB\nCollected 160,000 docs, 0.722 GB\nCollected 180,000 docs, 0.811 GB\nCollected 200,000 docs, 0.902 GB\nCollected 220,000 docs, 0.991 GB\nSelected 238,301 documents, approx 1.074 GB\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4197950395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Selected {len(texts):,} documents, approx {acc_bytes/1e9:.3f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mraw_text_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"nisram_hi_raw.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'OUT_DIR' is not defined"],"ename":"NameError","evalue":"name 'OUT_DIR' is not defined","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"raw_text_file = OUT_DIR / \"nisram_hi_raw.txt\"\nwith open(raw_text_file, \"w\", encoding=\"utf-8\") as f:\n    for t in texts:\n        line = t.replace(\"\\r\\n\", \"\\n\").strip()\n        if line == \"\":\n            continue\n        f.write(line + \"\\n\")\nprint(\"Saved raw text file:\", raw_text_file, \"size GB:\", raw_text_file.stat().st_size / 1e9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:56:19.437813Z","iopub.execute_input":"2025-12-03T05:56:19.438567Z","iopub.status.idle":"2025-12-03T05:56:22.196092Z","shell.execute_reply.started":"2025-12-03T05:56:19.438529Z","shell.execute_reply":"2025-12-03T05:56:22.195297Z"}},"outputs":[{"name":"stdout","text":"Saved raw text file: /kaggle/working/hindi-gpt/nisram_hi_raw.txt size GB: 1.073857009\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# -------------------- TRAIN BYTE-LEVEL BPE TOKENIZER --------------------\nprint(\"Training ByteLevel BPE tokenizer (vocab_size = {}) ...\".format(TEACHER_VOCAB_SIZE))\nbpe = ByteLevelBPETokenizer()\nbpe.train(files=[str(raw_text_file)], vocab_size=TEACHER_VOCAB_SIZE, min_frequency=2,\n          special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\nbpe.save_model(str(TK_DIR))\nprint(\"Tokenizer model files saved to:\", TK_DIR)\n\n# wrap as HuggingFace fast tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained(str(TK_DIR))\ntokenizer.add_special_tokens({\n    \"bos_token\": \"<s>\",\n    \"eos_token\": \"</s>\",\n    \"unk_token\": \"<unk>\",\n    \"pad_token\": \"<pad>\",\n})\nprint(\"Tokenizer vocab size reported:\", tokenizer.vocab_size)\n\n# -------------------- TOKENIZE & BUILD 1D TOKEN STREAM --------------------\nprint(\"Tokenizing texts and building token stream ...\")\nall_ids = []\nbatch_tokenize = 500\nfor i in range(0, len(texts), batch_tokenize):\n    batch = texts[i:i+batch_tokenize]\n    enc = tokenizer(batch, add_special_tokens=False)\n    for seq in enc[\"input_ids\"]:\n        all_ids.extend(seq + [tokenizer.eos_token_id])\n    if (i // batch_tokenize) % 20 == 0:\n        print(f\"Tokenized up to doc {i+len(batch):,}, tokens so far: {len(all_ids):,}\")\n\ndata = torch.tensor(all_ids, dtype=torch.long)\nprint(\"Total tokens in dataset:\", len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T05:57:08.729219Z","iopub.execute_input":"2025-12-03T05:57:08.729768Z","iopub.status.idle":"2025-12-03T06:05:55.765079Z","shell.execute_reply.started":"2025-12-03T05:57:08.729745Z","shell.execute_reply":"2025-12-03T06:05:55.764379Z"}},"outputs":[{"name":"stdout","text":"Training ByteLevel BPE tokenizer (vocab_size = 32000) ...\n\n\n\nTokenizer model files saved to: /kaggle/working/hindi-gpt/tokenizer\nTokenizer vocab size reported: 32000\nTokenizing texts and building token stream ...\nTokenized up to doc 500, tokens so far: 580,802\nTokenized up to doc 10,500, tokens so far: 11,988,665\nTokenized up to doc 20,500, tokens so far: 23,357,119\nTokenized up to doc 30,500, tokens so far: 34,779,078\nTokenized up to doc 40,500, tokens so far: 46,118,471\nTokenized up to doc 50,500, tokens so far: 57,335,192\nTokenized up to doc 60,500, tokens so far: 68,695,945\nTokenized up to doc 70,500, tokens so far: 80,066,706\nTokenized up to doc 80,500, tokens so far: 91,339,023\nTokenized up to doc 90,500, tokens so far: 102,694,022\nTokenized up to doc 100,500, tokens so far: 114,028,601\nTokenized up to doc 110,500, tokens so far: 125,344,266\nTokenized up to doc 120,500, tokens so far: 136,747,126\nTokenized up to doc 130,500, tokens so far: 148,175,479\nTokenized up to doc 140,500, tokens so far: 159,516,951\nTokenized up to doc 150,500, tokens so far: 170,705,413\nTokenized up to doc 160,500, tokens so far: 181,984,413\nTokenized up to doc 170,500, tokens so far: 193,288,906\nTokenized up to doc 180,500, tokens so far: 204,534,009\nTokenized up to doc 190,500, tokens so far: 215,892,083\nTokenized up to doc 200,500, tokens so far: 227,189,020\nTokenized up to doc 210,500, tokens so far: 238,415,610\nTokenized up to doc 220,500, tokens so far: 249,701,908\nTokenized up to doc 230,500, tokens so far: 261,127,117\nTotal tokens in dataset: 269922676\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# -------------------- TRAIN / VAL SPLIT --------------------\nn = int(0.9 * len(data))\ntrain_data = data[:n].to(device)\nval_data = data[n:].to(device)\nprint(\"Train tokens:\", len(train_data), \"Val tokens:\", len(val_data))\n\n# -------------------- BATCH / GET_BATCH --------------------\ndef get_batch(split):\n    data_local = train_data if split == \"train\" else val_data\n    max_start = len(data_local) - BLOCK_SIZE - 1\n    ix = torch.randint(0, max_start, (BATCH_SIZE,))\n    x = torch.stack([data_local[i:i+BLOCK_SIZE] for i in ix])\n    y = torch.stack([data_local[i+1:i+BLOCK_SIZE+1] for i in ix])\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:05:55.786515Z","iopub.execute_input":"2025-12-03T06:05:55.786745Z","iopub.status.idle":"2025-12-03T06:05:56.402388Z","shell.execute_reply.started":"2025-12-03T06:05:55.786728Z","shell.execute_reply":"2025-12-03T06:05:56.401761Z"}},"outputs":[{"name":"stdout","text":"Train tokens: 242930408 Val tokens: 26992268\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# -------------------- TEACHER MODEL DEFINITION --------------------\nclass TeacherHead(nn.Module):\n    def __init__(self, d_model, head_size, block_size, dropout):\n        super().__init__()\n        self.key = nn.Linear(d_model, head_size, bias=False)\n        self.query = nn.Linear(d_model, head_size, bias=False)\n        self.value = nn.Linear(d_model, head_size, bias=False)\n\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n\n        wei = q @ k.transpose(-2, -1) * (q.size(-1) ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n\n        v = self.value(x)\n        return wei @ v\n\n\nclass TeacherMultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_head, head_size, dropout, block_size):\n        super().__init__()\n        self.heads = nn.ModuleList([\n            TeacherHead(d_model, head_size, block_size, dropout)\n            for _ in range(n_head)\n        ])\n\n        self.proj = nn.Linear(n_head * head_size, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(out))\n\n\nclass TeacherFeedForward(nn.Module):\n    def __init__(self, d_model, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TeacherBlock(nn.Module):\n    def __init__(self, d_model, n_head, block_size, dropout):\n        super().__init__()\n        head_size = d_model // n_head\n\n        self.sa = TeacherMultiHeadAttention(\n            d_model, n_head, head_size, dropout, block_size\n        )\n        self.ffwd = TeacherFeedForward(d_model, dropout)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\nclass TeacherGPT(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 block_size,\n                 d_model,\n                 n_head,\n                 n_layer,\n                 dropout):\n\n        super().__init__()\n\n        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n        self.position_embedding_table = nn.Embedding(block_size, d_model)\n\n        self.blocks = nn.Sequential(\n            *[TeacherBlock(d_model, n_head, block_size, dropout) for _ in range(n_layer)]\n        )\n\n        self.ln_f = nn.LayerNorm(d_model)\n        self.lm_head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n        x = tok_emb + pos_emb\n\n        x = self.blocks(x)\n        x = self.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.view(B*T, -1),\n                targets.view(B*T),\n                reduction=\"mean\"\n            )\n\n        return logits, loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:21.063383Z","iopub.execute_input":"2025-12-03T06:17:21.064017Z","iopub.status.idle":"2025-12-03T06:17:21.076770Z","shell.execute_reply.started":"2025-12-03T06:17:21.063997Z","shell.execute_reply":"2025-12-03T06:17:21.076022Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"teacher = TeacherGPT(\n    vocab_size=32000,\n    d_model=512,\n    n_head=8,\n    n_layer=8,\n    block_size=128,\n    dropout=0.1\n)\n\n\ncheckpoint = torch.load(teacher_ckpt_path, map_location=\"cpu\")\nteacher.load_state_dict(checkpoint, strict=True)\nteacher = teacher.to(device)\n\nteacher.eval()\nprint(\"Teacher model loaded\")\nfor p in teacher.parameters():\n    p.requires_grad = False\n\nteacher_params = sum(p.numel() for p in teacher.parameters())\nprint(f\"âœ“ Teacher loaded: {teacher_params:,} parameters ({teacher_params/1e6:.2f}M)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:23:17.582862Z","iopub.execute_input":"2025-12-03T06:23:17.583109Z","iopub.status.idle":"2025-12-03T06:23:18.332611Z","shell.execute_reply.started":"2025-12-03T06:23:17.583092Z","shell.execute_reply":"2025-12-03T06:23:18.331970Z"}},"outputs":[{"name":"stdout","text":"Teacher model loaded\nâœ“ Teacher loaded: 58,073,344 parameters (58.07M)\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# -------------------- STUDENT MODEL DEFINITION (GPT-style) --------------------\nclass StudentAFTSimpleHead(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.hidden_dim = head_size\n        self.to_q = nn.Linear(STUDENT_D_MODEL, head_size, bias=False)\n        self.to_k = nn.Linear(STUDENT_D_MODEL, head_size, bias=False)\n        self.to_v = nn.Linear(STUDENT_D_MODEL, head_size, bias=False)\n        self.project = nn.Linear(head_size, head_size)\n\n    def forward(self, x):\n        B, T, _ = x.shape\n        Q = self.to_q(x)\n        K = self.to_k(x)\n        V = self.to_v(x)\n\n        K_soft = F.softmax(K, dim=1)\n        weights = (K_soft * V).sum(dim=1, keepdim=True)\n        Q_sig = torch.sigmoid(Q)\n        Yt = Q_sig * weights\n\n        return self.project(Yt)\n\nclass StudentAFTSimpleMultiHead(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([StudentAFTSimpleHead(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(num_heads * head_size, STUDENT_D_MODEL)\n        self.dropout = nn.Dropout(STUDENT_DROPOUT)\n\n    def forward(self, x):\n        head_outs = [h(x) for h in self.heads]\n        out = torch.cat(head_outs, dim=-1)\n        out = self.proj(out)\n        return self.dropout(out)\n\nclass StudentFeedForward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(STUDENT_DROPOUT),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass StudentBlock(nn.Module):\n    def __init__(self, d_model, n_head):\n        super().__init__()\n        head_size = d_model // n_head\n        self.sa = StudentAFTSimpleMultiHead(n_head, head_size)\n        self.ffwd = StudentFeedForward(d_model)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ln2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass StudentGPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.token_embedding_table = nn.Embedding(STUDENT_VOCAB_SIZE, STUDENT_D_MODEL)\n        self.position_embedding_table = nn.Embedding(STUDENT_BLOCK_SIZE, STUDENT_D_MODEL)\n\n        self.blocks = nn.Sequential(\n            *[StudentBlock(STUDENT_D_MODEL, STUDENT_N_HEAD) for _ in range(STUDENT_N_LAYER)]\n        )\n\n        self.ln_f = nn.LayerNorm(STUDENT_D_MODEL)\n        self.lm_head = nn.Linear(STUDENT_D_MODEL, STUDENT_VOCAB_SIZE)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx)\n        pos_indices = torch.arange(T, device=idx.device)\n        pos_emb = self.position_embedding_table(pos_indices)\n        x = tok_emb + pos_emb.unsqueeze(0)\n\n        x = self.blocks(x)\n        x = self.ln_f(x)\n\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.view(B * T, STUDENT_VOCAB_SIZE),\n                targets.view(B * T)\n            )\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens, temperature=1.0):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -STUDENT_BLOCK_SIZE:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / max(1e-8, temperature)\n            probs = F.softmax(logits, dim=-1)\n            next_id = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, next_id), dim=1)\n        return idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:22.962827Z","iopub.execute_input":"2025-12-03T06:17:22.963057Z","iopub.status.idle":"2025-12-03T06:17:22.977259Z","shell.execute_reply.started":"2025-12-03T06:17:22.963032Z","shell.execute_reply":"2025-12-03T06:17:22.976373Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model = StudentGPT()\nmodel.to(\"cuda\")       # or \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:23.609482Z","iopub.execute_input":"2025-12-03T06:17:23.610181Z","iopub.status.idle":"2025-12-03T06:17:23.877267Z","shell.execute_reply.started":"2025-12-03T06:17:23.610161Z","shell.execute_reply":"2025-12-03T06:17:23.876700Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"StudentGPT(\n  (token_embedding_table): Embedding(32000, 256)\n  (position_embedding_table): Embedding(128, 256)\n  (blocks): Sequential(\n    (0): StudentBlock(\n      (sa): StudentAFTSimpleMultiHead(\n        (heads): ModuleList(\n          (0-3): 4 x StudentAFTSimpleHead(\n            (to_q): Linear(in_features=256, out_features=64, bias=False)\n            (to_k): Linear(in_features=256, out_features=64, bias=False)\n            (to_v): Linear(in_features=256, out_features=64, bias=False)\n            (project): Linear(in_features=64, out_features=64, bias=True)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): StudentFeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): StudentBlock(\n      (sa): StudentAFTSimpleMultiHead(\n        (heads): ModuleList(\n          (0-3): 4 x StudentAFTSimpleHead(\n            (to_q): Linear(in_features=256, out_features=64, bias=False)\n            (to_k): Linear(in_features=256, out_features=64, bias=False)\n            (to_v): Linear(in_features=256, out_features=64, bias=False)\n            (project): Linear(in_features=64, out_features=64, bias=True)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): StudentFeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): StudentBlock(\n      (sa): StudentAFTSimpleMultiHead(\n        (heads): ModuleList(\n          (0-3): 4 x StudentAFTSimpleHead(\n            (to_q): Linear(in_features=256, out_features=64, bias=False)\n            (to_k): Linear(in_features=256, out_features=64, bias=False)\n            (to_v): Linear(in_features=256, out_features=64, bias=False)\n            (project): Linear(in_features=64, out_features=64, bias=True)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): StudentFeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): StudentBlock(\n      (sa): StudentAFTSimpleMultiHead(\n        (heads): ModuleList(\n          (0-3): 4 x StudentAFTSimpleHead(\n            (to_q): Linear(in_features=256, out_features=64, bias=False)\n            (to_k): Linear(in_features=256, out_features=64, bias=False)\n            (to_v): Linear(in_features=256, out_features=64, bias=False)\n            (project): Linear(in_features=64, out_features=64, bias=True)\n          )\n        )\n        (proj): Linear(in_features=256, out_features=256, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ffwd): StudentFeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=256, out_features=1024, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1024, out_features=256, bias=True)\n          (3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=256, out_features=32000, bias=True)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"student_params = sum(p.numel() for p in model.parameters())\ncompression_ratio = teacher_params / student_params\nprint(f\"âœ“ Student created: {student_params:,} parameters ({student_params/1e6:.2f}M)\")\nprint(f\"ðŸ“Š Compression ratio: {compression_ratio:.2f}x smaller\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:30.137640Z","iopub.execute_input":"2025-12-03T06:17:30.138135Z","iopub.status.idle":"2025-12-03T06:17:30.142913Z","shell.execute_reply.started":"2025-12-03T06:17:30.138116Z","shell.execute_reply":"2025-12-03T06:17:30.142259Z"}},"outputs":[{"name":"stdout","text":"âœ“ Student created: 19,671,808 parameters (19.67M)\nðŸ“Š Compression ratio: 2.95x smaller\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# KD hyperparameters\nTEMPERATURE = 2.0\nALPHA = 0.5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:33.176603Z","iopub.execute_input":"2025-12-03T06:17:33.177066Z","iopub.status.idle":"2025-12-03T06:17:33.180750Z","shell.execute_reply.started":"2025-12-03T06:17:33.177043Z","shell.execute_reply":"2025-12-03T06:17:33.179862Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def kd_loss(student_logits, teacher_logits, targets,\n            T=TEMPERATURE, alpha=ALPHA):\n    \"\"\"\n    KD loss = alpha * hard CE + (1-alpha) * soft KL.\n\n    student_logits: (B, T, V)\n    teacher_logits: (B, T, V)\n    targets      : (B, T)\n    \"\"\"\n    B, Tseq, V = student_logits.shape\n\n    # Hard loss: CE over all tokens (mean per token)\n    ce = F.cross_entropy(\n        student_logits.view(B * Tseq, V),\n        targets.view(B * Tseq)\n    )\n\n    # Soft loss: KL over all tokens (mean per token)\n    student_logp = F.log_softmax(\n        student_logits.view(B * Tseq, V) / T, dim=-1\n    )\n    teacher_soft = F.softmax(\n        teacher_logits.view(B * Tseq, V) / T, dim=-1\n    )\n\n    kl = F.kl_div(\n        student_logp,\n        teacher_soft,\n        reduction=\"batchmean\",  # now truly mean over B*T tokens\n    ) * (T * T)\n\n    return alpha * ce + (1.0 - alpha) * kl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:17:33.605682Z","iopub.execute_input":"2025-12-03T06:17:33.606366Z","iopub.status.idle":"2025-12-03T06:17:33.611192Z","shell.execute_reply.started":"2025-12-03T06:17:33.606344Z","shell.execute_reply":"2025-12-03T06:17:33.610471Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.amp.GradScaler(enabled=(device==\"cuda\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:18:12.173628Z","iopub.execute_input":"2025-12-03T06:18:12.174286Z","iopub.status.idle":"2025-12-03T06:18:12.178758Z","shell.execute_reply.started":"2025-12-03T06:18:12.174265Z","shell.execute_reply":"2025-12-03T06:18:12.177954Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# -------------------- EVAL / METRICS --------------------\n@torch.no_grad()\ndef estimate_loss():\n    was_training = model.training\n    model.eval()\n    out = {}\n    for split in [\"train\", \"val\"]:\n        losses = []\n        iters = min(EVAL_ITERS,  max(1, (len(train_data) if split==\"train\" else len(val_data)) // (BATCH_SIZE * 2)))\n        for _ in range(iters):\n            xb, yb = get_batch(split)\n            xb = xb.to(device); yb = yb.to(device)\n            if device == \"cuda\":\n                with torch.cuda.amp.autocast():\n                    _, loss = model(xb, yb)\n            else:\n                _, loss = model(xb, yb)\n            losses.append(loss.mean().item())\n        out[split] = float(np.mean(losses))\n    if was_training:\n        model.train()\n    return out\n\ndef ppl_from_loss(loss_val):\n    return math.exp(loss_val) if loss_val < 50 else float(\"inf\")\n\n# -------------------- GENERATION (greedy helper) --------------------\n@torch.no_grad()\ndef generate_text(prompt, max_new_tokens=200):\n    model.eval()\n    tokens = tokenizer.encode(prompt)\n    idx = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n    idx = model.generate(idx, max_new_tokens=max_new_tokens, temperature=1.0)\n    text = tokenizer.decode(idx.squeeze().tolist(), skip_special_tokens=True)\n    model.train()\n    return text\n# -------------------- LOGGING SETUP (ensure these exist) --------------------\nlog_csv = LOG_DIR / \"train_log.csv\"\n\n# If starting fresh, create CSV header\nif not log_csv.exists():\n    with open(log_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"step\",\"train_loss_iter\",\"eval_train_loss\",\"eval_val_loss\",\"eval_train_ppl\",\"eval_val_ppl\",\"step_time_s\",\"save_time_s\",\"timestamp\"])\n\n# -------------------- RESET LOGGING LISTS --------------------\nloss_history = []\neval_history = []\nperp_history = []\ntimings = []\n\n# -------------------- TRAIN LOOP WITH KD --------------------\nprint(\"Starting KD training for\", TOTAL_STEPS, \"steps ...\")\nglobal_start = time.time()\nstep = 0\n\ntry:\n    init = estimate_loss()\n    print(f\"Initial eval - Train {init['train']:.4f}, Val {init['val']:.4f}\")\nexcept Exception as e:\n    print(\"Initial eval failed:\", e)\n    init = {\"train\": None, \"val\": None}\n\nmodel.train()   # student\nteacher.eval()  # teacher stays frozen\n\nwhile step < TOTAL_STEPS:\n    step += 1\n    t0 = time.time()\n\n    xb, yb = get_batch(\"train\")\n    xb = xb.to(device)\n    yb = yb.to(device)\n\n    # Teacher forward\n    with torch.no_grad():\n        teacher_logits, _ = teacher(xb, None)\n\n    # Student forward\n    if device == \"cuda\":\n        with torch.cuda.amp.autocast():\n            student_logits, _ = model(xb, None)\n            loss_raw = kd_loss(student_logits, teacher_logits, yb)\n    else:\n        student_logits, _ = model(xb, None)\n        loss_raw = kd_loss(student_logits, teacher_logits, yb)\n\n    loss = loss_raw.mean()\n    loss_to_backprop = loss / (GRAD_ACCUM_STEPS if USE_GRAD_ACCUM else 1)\n\n    # Backprop\n    if device == \"cuda\":\n        scaler.scale(loss_to_backprop).backward()\n    else:\n        loss_to_backprop.backward()\n\n    # Optimizer step\n    if (not USE_GRAD_ACCUM) or (step % GRAD_ACCUM_STEPS == 0):\n        if device == \"cuda\":\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n\n    step_time = time.time() - t0\n    iter_train_loss = float(loss.item())\n    loss_history.append((step, iter_train_loss))\n\n    # Eval and checkpoint\n    save_time_s = \"\"\n    train_loss_eval = None\n    val_loss_eval = None\n    val_ppl = None\n\n    if step % EVAL_INTERVAL == 0 or step == TOTAL_STEPS:\n        t_eval_start = time.time()\n        metrics = estimate_loss()\n        t_eval = time.time() - t_eval_start\n\n        train_loss_eval = metrics.get(\"train\")\n        val_loss_eval = metrics.get(\"val\")\n        train_ppl = ppl_from_loss(train_loss_eval)\n        val_ppl = ppl_from_loss(val_loss_eval)\n\n        print(f\"[Step {step}] KD iter_loss {iter_train_loss:.4f} | Eval Train {train_loss_eval:.4f} Val {val_loss_eval:.4f} | ValPPL {val_ppl:.2f} | eval_time {t_eval:.1f}s\")\n        eval_history.append((step, train_loss_eval, val_loss_eval))\n        perp_history.append((step, val_ppl))\n\n    if step % SAVE_EVERY == 0 or step == TOTAL_STEPS:\n        t_save_start = time.time()\n        model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n        ckpt_path = CKPT_DIR / f\"gpt2_student_step{step}.pt\"\n        torch.save(model_to_save.state_dict(), ckpt_path)\n        tokenizer.save_pretrained(str(TK_DIR))\n        save_time_s = time.time() - t_save_start\n        print(f\"Saved checkpoint step {step} (save_time {save_time_s:.2f}s)\")\n\n    # Logging\n    with open(log_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f)\n        row = [\n            step,\n            iter_train_loss,\n            train_loss_eval if train_loss_eval is not None else \"\",\n            val_loss_eval if val_loss_eval is not None else \"\",\n            ppl_from_loss(train_loss_eval) if train_loss_eval is not None else \"\",\n            val_ppl if val_ppl is not None else \"\",\n            round(step_time, 6),\n            save_time_s,\n            datetime.utcnow().isoformat()\n        ]\n        writer.writerow(row)\n\n    timings.append({\"step\": step, \"step_time_s\": step_time, \"loss\": iter_train_loss, \"save_time_s\": save_time_s})\n\n# End KD training\ntotal_time = time.time() - global_start\nprint(\"KD training finished. Total time (s):\", total_time)\n\n# Final save\nt_final_save_start = time.time()\nmodel_to_save = model.module if isinstance(model, nn.DataParallel) else model\nfinal_path = CKPT_DIR / \"gpt2_student_final.pt\"\ntorch.save(model_to_save.state_dict(), final_path)\ntokenizer.save_pretrained(str(TK_DIR))\nt_final_save = time.time() - t_final_save_start\nprint(\"Final student model saved (s):\", t_final_save)\n\n# -------------------- INFERENCE TIMING --------------------\nprint(\"Generating sample for prompt (timing)...\")\nt_inf_start = time.time()\ngenerated = generate_text(INFERENCE_PROMPT, max_new_tokens=GENERATE_TOKENS)\nt_inf = time.time() - t_inf_start\nprint(\"Inference time (s) for {} tokens: {:.3f}s\".format(GENERATE_TOKENS, t_inf))\n\nwith open(OUT_DIR / \"sample_generation.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"PROMPT:\\n\" + INFERENCE_PROMPT + \"\\n\\n\")\n    f.write(\"GENERATED:\\n\" + generated + \"\\n\\n\")\n    f.write(f\"Inference time (s): {t_inf}\\n\")\n\n# -------------------- SUMMARY --------------------\nsummary = {\n    \"total_steps\": TOTAL_STEPS,\n    \"batch_size\": BATCH_SIZE,\n\n    \"student_block_size\": STUDENT_BLOCK_SIZE,\n    \"student_d_model\": STUDENT_D_MODEL,\n    \"student_n_head\": STUDENT_N_HEAD,\n    \"student_n_layer\": STUDENT_N_LAYER,\n    \"student_vocab\": STUDENT_VOCAB_SIZE,\n\n    \"teacher_block_size\": TEACHER_BLOCK_SIZE,\n    \"teacher_d_model\": TEACHER_D_MODEL,\n    \"teacher_n_head\": TEACHER_N_HEAD,\n    \"teacher_n_layer\": TEACHER_N_LAYER,\n    \"teacher_vocab\": TEACHER_VOCAB_SIZE,\n\n    \"kd_temperature\": TEMPERATURE,\n    \"kd_alpha\": ALPHA,\n\n    \"student_parameters\": student_params,\n    \"teacher_parameters\": teacher_params,\n    \"compression_ratio\": compression_ratio,\n\n    \"final_checkpoint\": str(final_path),\n    \"teacher_checkpoint_used\": teacher_ckpt_path,\n    \"tokenizer_dir\": str(TK_DIR),\n    \"loss_plot\": str(OUT_DIR / \"loss_curve.png\"),\n    \"perplexity_plot\": str(OUT_DIR / \"perplexity_curve.png\"),\n    \"timings_csv\": str(OUT_DIR / \"timings.csv\"),\n    \"inference_time_s\": t_inf\n}\n\nprint(\"SUMMARY REPORT\")\nfor key, value in summary.items():\n    print(f\"  {key}: {value}\")\n\nprint(\"All outputs saved under:\", OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T06:25:15.486038Z","iopub.execute_input":"2025-12-03T06:25:15.486313Z","iopub.status.idle":"2025-12-03T07:43:24.137244Z","shell.execute_reply.started":"2025-12-03T06:25:15.486292Z","shell.execute_reply":"2025-12-03T07:43:24.136492Z"}},"outputs":[{"name":"stdout","text":"Starting KD training for 50000 steps ...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1600034413.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Initial eval - Train 3.9959, Val 3.9951\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1600034413.py:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"[Step 2000] KD iter_loss 3.7527 | Eval Train 3.9459 Val 3.9651 | ValPPL 52.73 | eval_time 3.1s\nSaved checkpoint step 2000 (save_time 0.15s)\n[Step 4000] KD iter_loss 3.5527 | Eval Train 3.8347 Val 3.8370 | ValPPL 46.39 | eval_time 3.1s\nSaved checkpoint step 4000 (save_time 0.16s)\n[Step 6000] KD iter_loss 3.2876 | Eval Train 3.7038 Val 3.6842 | ValPPL 39.81 | eval_time 3.1s\nSaved checkpoint step 6000 (save_time 0.14s)\n[Step 8000] KD iter_loss 3.1943 | Eval Train 3.5837 Val 3.5872 | ValPPL 36.13 | eval_time 3.1s\nSaved checkpoint step 8000 (save_time 0.16s)\n[Step 10000] KD iter_loss 3.0537 | Eval Train 3.4676 Val 3.4449 | ValPPL 31.34 | eval_time 3.1s\nSaved checkpoint step 10000 (save_time 0.16s)\n[Step 12000] KD iter_loss 2.9655 | Eval Train 3.3338 Val 3.3362 | ValPPL 28.11 | eval_time 3.1s\nSaved checkpoint step 12000 (save_time 0.14s)\n[Step 14000] KD iter_loss 2.9344 | Eval Train 3.1590 Val 3.1677 | ValPPL 23.75 | eval_time 3.1s\nSaved checkpoint step 14000 (save_time 0.14s)\n[Step 16000] KD iter_loss 2.6083 | Eval Train 2.9453 Val 2.9350 | ValPPL 18.82 | eval_time 3.1s\nSaved checkpoint step 16000 (save_time 0.14s)\n[Step 18000] KD iter_loss 2.4087 | Eval Train 2.6818 Val 2.6774 | ValPPL 14.55 | eval_time 3.1s\nSaved checkpoint step 18000 (save_time 0.13s)\n[Step 20000] KD iter_loss 2.2620 | Eval Train 2.4732 Val 2.4741 | ValPPL 11.87 | eval_time 3.1s\nSaved checkpoint step 20000 (save_time 0.14s)\n[Step 22000] KD iter_loss 2.0710 | Eval Train 2.2888 Val 2.2862 | ValPPL 9.84 | eval_time 3.1s\nSaved checkpoint step 22000 (save_time 0.14s)\n[Step 24000] KD iter_loss 1.9332 | Eval Train 2.1409 Val 2.1346 | ValPPL 8.45 | eval_time 3.1s\nSaved checkpoint step 24000 (save_time 0.14s)\n[Step 26000] KD iter_loss 1.9216 | Eval Train 2.0017 Val 2.0103 | ValPPL 7.47 | eval_time 3.1s\nSaved checkpoint step 26000 (save_time 0.14s)\n[Step 28000] KD iter_loss 1.7680 | Eval Train 1.8920 Val 1.8796 | ValPPL 6.55 | eval_time 3.1s\nSaved checkpoint step 28000 (save_time 0.15s)\n[Step 30000] KD iter_loss 1.7027 | Eval Train 1.7422 Val 1.7532 | ValPPL 5.77 | eval_time 3.1s\nSaved checkpoint step 30000 (save_time 0.14s)\n[Step 32000] KD iter_loss 1.6540 | Eval Train 1.6055 Val 1.6004 | ValPPL 4.96 | eval_time 3.1s\nSaved checkpoint step 32000 (save_time 0.14s)\n[Step 34000] KD iter_loss 1.4730 | Eval Train 1.4897 Val 1.4865 | ValPPL 4.42 | eval_time 3.1s\nSaved checkpoint step 34000 (save_time 0.13s)\n[Step 36000] KD iter_loss 1.3324 | Eval Train 1.3304 Val 1.3479 | ValPPL 3.85 | eval_time 3.1s\nSaved checkpoint step 36000 (save_time 0.13s)\n[Step 38000] KD iter_loss 1.4089 | Eval Train 1.2436 Val 1.2603 | ValPPL 3.53 | eval_time 3.1s\nSaved checkpoint step 38000 (save_time 0.13s)\n[Step 40000] KD iter_loss 1.3703 | Eval Train 1.1935 Val 1.1908 | ValPPL 3.29 | eval_time 3.1s\nSaved checkpoint step 40000 (save_time 0.14s)\n[Step 42000] KD iter_loss 1.2601 | Eval Train 1.1075 Val 1.1218 | ValPPL 3.07 | eval_time 3.1s\nSaved checkpoint step 42000 (save_time 0.14s)\n[Step 44000] KD iter_loss 1.2093 | Eval Train 1.0593 Val 1.0672 | ValPPL 2.91 | eval_time 3.1s\nSaved checkpoint step 44000 (save_time 0.13s)\n[Step 46000] KD iter_loss 1.1994 | Eval Train 1.0120 Val 1.0052 | ValPPL 2.73 | eval_time 3.1s\nSaved checkpoint step 46000 (save_time 0.14s)\n[Step 48000] KD iter_loss 1.1804 | Eval Train 1.0064 Val 0.9915 | ValPPL 2.70 | eval_time 3.1s\nSaved checkpoint step 48000 (save_time 0.15s)\n[Step 50000] KD iter_loss 1.1562 | Eval Train 0.9513 Val 0.9652 | ValPPL 2.63 | eval_time 3.1s\nSaved checkpoint step 50000 (save_time 0.13s)\nKD training finished. Total time (s): 4687.269647598267\nFinal student model saved (s): 0.14561820030212402\nGenerating sample for prompt (timing)...\nInference time (s) for 200 tokens: 1.215s\nSUMMARY REPORT\n  total_steps: 50000\n  batch_size: 16\n  student_block_size: 128\n  student_d_model: 256\n  student_n_head: 4\n  student_n_layer: 4\n  student_vocab: 32000\n  teacher_block_size: 128\n  teacher_d_model: 512\n  teacher_n_head: 8\n  teacher_n_layer: 8\n  teacher_vocab: 32000\n  kd_temperature: 2.0\n  kd_alpha: 0.5\n  student_parameters: 19671808\n  teacher_parameters: 58073344\n  compression_ratio: 2.952110146662676\n  final_checkpoint: /kaggle/working/hindi-gpt/checkpoints/gpt2_student_final.pt\n  teacher_checkpoint_used: /kaggle/input/teacher-hindi/pytorch/default/1/gpt2_hindi_step50000.pt\n  tokenizer_dir: /kaggle/working/hindi-gpt/tokenizer\n  loss_plot: /kaggle/working/hindi-gpt/loss_curve.png\n  perplexity_plot: /kaggle/working/hindi-gpt/perplexity_curve.png\n  timings_csv: /kaggle/working/hindi-gpt/timings.csv\n  inference_time_s: 1.2147133350372314\nAll outputs saved under: /kaggle/working/hindi-gpt\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}